{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Content<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load\" data-toc-modified-id=\"Load-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load</a></span></li><li><span><a href=\"#Calculate-Term-Frequencies\" data-toc-modified-id=\"Calculate-Term-Frequencies-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Calculate Term Frequencies</a></span></li><li><span><a href=\"#Build-keras-model\" data-toc-modified-id=\"Build-keras-model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Build keras model</a></span></li><li><span><a href=\"#Train-and-save-our-model\" data-toc-modified-id=\"Train-and-save-our-model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Train and save our model</a></span></li><li><span><a href=\"#Testing-on-validation-set\" data-toc-modified-id=\"Testing-on-validation-set-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Testing on validation set</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:35:58.093634Z",
     "start_time": "2018-04-28T18:35:57.915778Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:35:58.165084Z",
     "start_time": "2018-04-28T18:35:58.094964Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv', index_col='time')\n",
    "test = pd.read_csv('data/test.csv', index_col='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:35:58.171180Z",
     "start_time": "2018-04-28T18:35:58.166591Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid = train[[col for col in train.columns if col!='severity']], train['severity'],\\\n",
    "                                     test[[col for col in train.columns if col!='severity']], test['severity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Term Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:35:58.450979Z",
     "start_time": "2018-04-28T18:35:58.172667Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "\n",
    "import scipy.sparse as sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:35:58.457055Z",
     "start_time": "2018-04-28T18:35:58.452475Z"
    }
   },
   "outputs": [],
   "source": [
    "def Vect(df, use_idf=True, min_df=1, max_df=1.0, ngram_range = (4,8)):\n",
    "    '''\n",
    "    Tfidf or tf vectorizing\n",
    "    \n",
    "    us_idf: bool([True, False]), use tfidf or not;\n",
    "    min_df: float()\n",
    "    max_df: float()\n",
    "    ngram_range: tuple(None, None)\n",
    "    '''\n",
    "    if use_idf == True:\n",
    "        tfidf = TfidfVectorizer(stop_words=stopwords.words('english'), preprocessor=None,\n",
    "                                ngram_range=ngram_range, strip_accents='ascii', analyzer='word',\n",
    "                                min_df = min_df, max_df=max_df, tokenizer=None)\n",
    "    \n",
    "        message_encoding = tfidf.fit_transform(df['message_encoding'])\n",
    "        \n",
    "        return message_encoding, tfidf\n",
    "    \n",
    "    else:\n",
    "        tf = CountVectorizer(stop_words=stopwords.words('english'), preprocessor=None,\n",
    "                             ngram_range=ngram_range, strip_accents='ascii', analyzer='word',\n",
    "                             min_df = min_df, max_df=max_df, tokenizer=None)\n",
    "\n",
    "        message_encoding = tf.fit_transform(df['message_encoding'])\n",
    "        \n",
    "        return message_encoding, tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:36:19.721260Z",
     "start_time": "2018-04-28T18:35:58.459152Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_idf, idf_m = Vect(X_train, max_df=0.80, min_df=30)\n",
    "X_valid_idf = idf_m.transform(X_valid['message_encoding'])\n",
    "\n",
    "X_train_tf, tf_m = Vect(X_train, use_idf=False, max_df=0.80, min_df=30)\n",
    "X_valid_tf = tf_m.transform(X_valid['message_encoding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:36:19.738485Z",
     "start_time": "2018-04-28T18:36:19.722772Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_idf = sps.hstack((X_train_idf, train[['author_name', 'author_email', 'committer_name', \n",
    "                                              'committer_email', 'tm_year', 'tm_mon', 'tm_mday', \n",
    "                                              'tm_hour', 'tm_wday', 'tm_yday', 'first_w']]))\n",
    "\n",
    "X_valid_idf = sps.hstack((X_valid_idf, test[['author_name', 'author_email', 'committer_name', \n",
    "                                             'committer_email', 'tm_year', 'tm_mon', 'tm_mday', \n",
    "                                             'tm_hour', 'tm_wday', 'tm_yday', 'first_w']]))\n",
    "\n",
    "X_train_tf = sps.hstack((X_train_tf, train[['author_name', 'author_email', 'committer_name', \n",
    "                                              'committer_email', 'tm_year', 'tm_mon', 'tm_mday', \n",
    "                                              'tm_hour', 'tm_wday', 'tm_yday', 'first_w']]))\n",
    "\n",
    "X_valid_tf = sps.hstack((X_valid_tf, test[['author_name', 'author_email', 'committer_name', \n",
    "                                           'committer_email', 'tm_year', 'tm_mon', 'tm_mday', \n",
    "                                           'tm_hour', 'tm_wday', 'tm_yday', 'first_w']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build keras model\n",
    "\n",
    "I chose the most popular architecture and adjust it a little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:36:20.985926Z",
     "start_time": "2018-04-28T18:36:19.739668Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denis/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout\n",
    "from keras.layers import LSTM, Activation\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:36:20.989785Z",
     "start_time": "2018-04-28T18:36:20.987291Z"
    }
   },
   "outputs": [],
   "source": [
    "# Embedding\n",
    "max_features = 500\n",
    "maxlen = 1344\n",
    "embedding_size = 32\n",
    "\n",
    "# Convolution\n",
    "filter_length = 4\n",
    "nb_filter = 32\n",
    "pool_length = 8\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:36:21.001256Z",
     "start_time": "2018-04-28T18:36:20.991627Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:36:21.264719Z",
     "start_time": "2018-04-28T18:36:21.002447Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denis/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=32, kernel_size=4, strides=1, padding=\"valid\")`\n",
      "  \n",
      "/home/denis/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=8)`\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "model.add(MaxPooling1D(pool_length=pool_length))\n",
    "model.add(LSTM(lstm_output_size))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[f1, recall, precision])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:36:21.268533Z",
     "start_time": "2018-04-28T18:36:21.266090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1344, 32)          16000     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1344, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1341, 32)          4128      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 167, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 150)               109800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 755       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 130,683\n",
      "Trainable params: 130,683\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:36:21.282463Z",
     "start_time": "2018-04-28T18:36:21.269831Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train_ohe = to_categorical(y_train)\n",
    "y_valid_ohe = to_categorical(y_valid)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T22:01:06.170314Z",
     "start_time": "2018-04-27T22:01:06.161072Z"
    },
    "scrolled": true
   },
   "source": [
    "def train(X, y, batch_size = 32, epochs = 10, name='Test'):\n",
    "    models = []\n",
    "    for i in [0,1,2,3,4]:\n",
    "        print ('Testing y == %d' % i)\n",
    "        file_path= name + \"best_weights_%d.h5\" % i\n",
    "        \n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_f1', \n",
    "                                     verbose=1, save_best_only=True, mode='max')\n",
    "        \n",
    "        early = EarlyStopping(monitor=\"val_f1\", mode=\"max\", patience=20)\n",
    "        \n",
    "        callbacks_list = [checkpoint, early]\n",
    "        model.fit(X.toarray(), y==i,\n",
    "                  validation_split=0.2,\n",
    "                  batch_size=batch_size, \n",
    "                  epochs=epochs, \n",
    "                  callbacks=callbacks_list)\n",
    "        models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:36:21.293162Z",
     "start_time": "2018-04-28T18:36:21.284019Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(X, y, batch_size = 64, epochs = 100, name='Test'):\n",
    "    file_path= name + \"best_weights.h5\"\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_f1', \n",
    "                                 verbose=1, save_best_only=True, mode='max')\n",
    "        \n",
    "    early = EarlyStopping(monitor=\"val_f1\", mode=\"max\", patience=30)\n",
    "        \n",
    "    callbacks_list = [checkpoint, early]\n",
    "    model.fit(X.toarray(), y,\n",
    "              validation_split=0.2,\n",
    "              batch_size=batch_size, \n",
    "              epochs=epochs, \n",
    "              callbacks=callbacks_list)\n",
    "    model.load_weights(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:50:49.002713Z",
     "start_time": "2018-04-28T18:36:21.294737Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8291 samples, validate on 2073 samples\n",
      "Epoch 1/100\n",
      "8291/8291 [==============================] - 18s 2ms/step - loss: 1.2828 - f1: 0.5278 - recall: 0.5959 - precision: 0.4885 - val_loss: 1.1113 - val_f1: 0.6093 - val_recall: 0.6541 - val_precision: 0.5717\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.60927, saving model to wieghts/tfidfbest_weights.h5\n",
      "Epoch 2/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.2280 - f1: 0.5498 - recall: 0.6623 - precision: 0.4718 - val_loss: 1.0992 - val_f1: 0.6102 - val_recall: 0.7516 - val_precision: 0.5148\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.60927 to 0.61023, saving model to wieghts/tfidfbest_weights.h5\n",
      "Epoch 3/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.2065 - f1: 0.5547 - recall: 0.6663 - precision: 0.4762 - val_loss: 1.1044 - val_f1: 0.6112 - val_recall: 0.7356 - val_precision: 0.5253\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.61023 to 0.61116, saving model to wieghts/tfidfbest_weights.h5\n",
      "Epoch 4/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1954 - f1: 0.5586 - recall: 0.6647 - precision: 0.4822 - val_loss: 1.0883 - val_f1: 0.6247 - val_recall: 0.7337 - val_precision: 0.5450\n",
      "\n",
      "Epoch 00004: val_f1 improved from 0.61116 to 0.62470, saving model to wieghts/tfidfbest_weights.h5\n",
      "Epoch 5/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1823 - f1: 0.5651 - recall: 0.6652 - precision: 0.4920 - val_loss: 1.0911 - val_f1: 0.6227 - val_recall: 0.7617 - val_precision: 0.5275\n",
      "\n",
      "Epoch 00005: val_f1 did not improve\n",
      "Epoch 6/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1754 - f1: 0.5630 - recall: 0.6665 - precision: 0.4879 - val_loss: 1.0784 - val_f1: 0.6265 - val_recall: 0.7472 - val_precision: 0.5406\n",
      "\n",
      "Epoch 00006: val_f1 improved from 0.62470 to 0.62654, saving model to wieghts/tfidfbest_weights.h5\n",
      "Epoch 7/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1666 - f1: 0.5684 - recall: 0.6602 - precision: 0.4996 - val_loss: 1.0890 - val_f1: 0.6215 - val_recall: 0.7424 - val_precision: 0.5358\n",
      "\n",
      "Epoch 00007: val_f1 did not improve\n",
      "Epoch 8/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1584 - f1: 0.5696 - recall: 0.6595 - precision: 0.5019 - val_loss: 1.0690 - val_f1: 0.6386 - val_recall: 0.7279 - val_precision: 0.5701\n",
      "\n",
      "Epoch 00008: val_f1 improved from 0.62654 to 0.63858, saving model to wieghts/tfidfbest_weights.h5\n",
      "Epoch 9/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1534 - f1: 0.5714 - recall: 0.6500 - precision: 0.5103 - val_loss: 1.0747 - val_f1: 0.6380 - val_recall: 0.7217 - val_precision: 0.5731\n",
      "\n",
      "Epoch 00009: val_f1 did not improve\n",
      "Epoch 10/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1454 - f1: 0.5770 - recall: 0.6537 - precision: 0.5170 - val_loss: 1.0771 - val_f1: 0.6375 - val_recall: 0.7188 - val_precision: 0.5737\n",
      "\n",
      "Epoch 00010: val_f1 did not improve\n",
      "Epoch 11/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1423 - f1: 0.5763 - recall: 0.6573 - precision: 0.5136 - val_loss: 1.0729 - val_f1: 0.6345 - val_recall: 0.7115 - val_precision: 0.5741\n",
      "\n",
      "Epoch 00011: val_f1 did not improve\n",
      "Epoch 12/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1375 - f1: 0.5742 - recall: 0.6561 - precision: 0.5111 - val_loss: 1.0823 - val_f1: 0.6283 - val_recall: 0.7082 - val_precision: 0.5660\n",
      "\n",
      "Epoch 00012: val_f1 did not improve\n",
      "Epoch 13/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1418 - f1: 0.5752 - recall: 0.6572 - precision: 0.5122 - val_loss: 1.0780 - val_f1: 0.6312 - val_recall: 0.7337 - val_precision: 0.5550\n",
      "\n",
      "Epoch 00013: val_f1 did not improve\n",
      "Epoch 14/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1349 - f1: 0.5802 - recall: 0.6711 - precision: 0.5120 - val_loss: 1.0881 - val_f1: 0.6244 - val_recall: 0.7241 - val_precision: 0.5511\n",
      "\n",
      "Epoch 00014: val_f1 did not improve\n",
      "Epoch 15/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1318 - f1: 0.5797 - recall: 0.6587 - precision: 0.5181 - val_loss: 1.0746 - val_f1: 0.6351 - val_recall: 0.7043 - val_precision: 0.5793\n",
      "\n",
      "Epoch 00015: val_f1 did not improve\n",
      "Epoch 16/100\n",
      "8291/8291 [==============================] - 15s 2ms/step - loss: 1.1211 - f1: 0.5805 - recall: 0.6629 - precision: 0.5170 - val_loss: 1.0786 - val_f1: 0.6369 - val_recall: 0.7342 - val_precision: 0.5640\n",
      "\n",
      "Epoch 00016: val_f1 did not improve\n",
      "Epoch 17/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1171 - f1: 0.5829 - recall: 0.6585 - precision: 0.5238 - val_loss: 1.0935 - val_f1: 0.6273 - val_recall: 0.7207 - val_precision: 0.5572\n",
      "\n",
      "Epoch 00017: val_f1 did not improve\n",
      "Epoch 18/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1179 - f1: 0.5822 - recall: 0.6680 - precision: 0.5166 - val_loss: 1.0870 - val_f1: 0.6297 - val_recall: 0.7077 - val_precision: 0.5688\n",
      "\n",
      "Epoch 00018: val_f1 did not improve\n",
      "Epoch 19/100\n",
      "8291/8291 [==============================] - 15s 2ms/step - loss: 1.1167 - f1: 0.5817 - recall: 0.6493 - precision: 0.5279 - val_loss: 1.0849 - val_f1: 0.6271 - val_recall: 0.7115 - val_precision: 0.5631\n",
      "\n",
      "Epoch 00019: val_f1 did not improve\n",
      "Epoch 20/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1171 - f1: 0.5834 - recall: 0.6625 - precision: 0.5221 - val_loss: 1.1131 - val_f1: 0.6114 - val_recall: 0.7053 - val_precision: 0.5420\n",
      "\n",
      "Epoch 00020: val_f1 did not improve\n",
      "Epoch 21/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1097 - f1: 0.5852 - recall: 0.6598 - precision: 0.5264 - val_loss: 1.0848 - val_f1: 0.6240 - val_recall: 0.6758 - val_precision: 0.5815\n",
      "\n",
      "Epoch 00021: val_f1 did not improve\n",
      "Epoch 22/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1075 - f1: 0.5849 - recall: 0.6532 - precision: 0.5301 - val_loss: 1.0998 - val_f1: 0.6208 - val_recall: 0.6725 - val_precision: 0.5784\n",
      "\n",
      "Epoch 00022: val_f1 did not improve\n",
      "Epoch 23/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1068 - f1: 0.5859 - recall: 0.6453 - precision: 0.5372 - val_loss: 1.0851 - val_f1: 0.6265 - val_recall: 0.6652 - val_precision: 0.5936\n",
      "\n",
      "Epoch 00023: val_f1 did not improve\n",
      "Epoch 24/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1017 - f1: 0.5852 - recall: 0.6367 - precision: 0.5421 - val_loss: 1.0984 - val_f1: 0.6245 - val_recall: 0.6768 - val_precision: 0.5817\n",
      "\n",
      "Epoch 00024: val_f1 did not improve\n",
      "Epoch 25/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1872 - f1: 0.5559 - recall: 0.5826 - precision: 0.5325 - val_loss: 1.0852 - val_f1: 0.6483 - val_recall: 0.6773 - val_precision: 0.6222\n",
      "\n",
      "Epoch 00025: val_f1 improved from 0.63858 to 0.64827, saving model to wieghts/tfidfbest_weights.h5\n",
      "Epoch 26/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1624 - f1: 0.5710 - recall: 0.5920 - precision: 0.5520 - val_loss: 1.0725 - val_f1: 0.6416 - val_recall: 0.6739 - val_precision: 0.6129\n",
      "\n",
      "Epoch 00026: val_f1 did not improve\n",
      "Epoch 27/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1474 - f1: 0.5722 - recall: 0.6049 - precision: 0.5434 - val_loss: 1.0688 - val_f1: 0.6366 - val_recall: 0.6845 - val_precision: 0.5958\n",
      "\n",
      "Epoch 00027: val_f1 did not improve\n",
      "Epoch 28/100\n",
      "8291/8291 [==============================] - 15s 2ms/step - loss: 1.1365 - f1: 0.5790 - recall: 0.6199 - precision: 0.5436 - val_loss: 1.0735 - val_f1: 0.6337 - val_recall: 0.6753 - val_precision: 0.5978\n",
      "\n",
      "Epoch 00028: val_f1 did not improve\n",
      "Epoch 29/100\n",
      "8291/8291 [==============================] - 15s 2ms/step - loss: 1.1257 - f1: 0.5762 - recall: 0.6097 - precision: 0.5469 - val_loss: 1.0728 - val_f1: 0.6273 - val_recall: 0.6700 - val_precision: 0.5911\n",
      "\n",
      "Epoch 00029: val_f1 did not improve\n",
      "Epoch 30/100\n",
      "8291/8291 [==============================] - 15s 2ms/step - loss: 1.1230 - f1: 0.5748 - recall: 0.5898 - precision: 0.5609 - val_loss: 1.1044 - val_f1: 0.6213 - val_recall: 0.6387 - val_precision: 0.6055\n",
      "\n",
      "Epoch 00030: val_f1 did not improve\n",
      "Epoch 31/100\n",
      "8291/8291 [==============================] - 15s 2ms/step - loss: 1.1220 - f1: 0.5732 - recall: 0.5908 - precision: 0.5571 - val_loss: 1.1718 - val_f1: 0.5700 - val_recall: 0.6836 - val_precision: 0.4915\n",
      "\n",
      "Epoch 00031: val_f1 did not improve\n",
      "Epoch 32/100\n",
      "8291/8291 [==============================] - 15s 2ms/step - loss: 1.1280 - f1: 0.5749 - recall: 0.6017 - precision: 0.5509 - val_loss: 1.0749 - val_f1: 0.6248 - val_recall: 0.6546 - val_precision: 0.5991\n",
      "\n",
      "Epoch 00032: val_f1 did not improve\n",
      "Epoch 33/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1189 - f1: 0.5771 - recall: 0.5951 - precision: 0.5608 - val_loss: 1.1046 - val_f1: 0.6201 - val_recall: 0.6368 - val_precision: 0.6050\n",
      "\n",
      "Epoch 00033: val_f1 did not improve\n",
      "Epoch 34/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1187 - f1: 0.5743 - recall: 0.6037 - precision: 0.5483 - val_loss: 1.0864 - val_f1: 0.6213 - val_recall: 0.6498 - val_precision: 0.5962\n",
      "\n",
      "Epoch 00034: val_f1 did not improve\n",
      "Epoch 35/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1134 - f1: 0.5827 - recall: 0.6265 - precision: 0.5451 - val_loss: 1.1529 - val_f1: 0.5992 - val_recall: 0.6546 - val_precision: 0.5544\n",
      "\n",
      "Epoch 00035: val_f1 did not improve\n",
      "Epoch 36/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1148 - f1: 0.5850 - recall: 0.6354 - precision: 0.5426 - val_loss: 1.1054 - val_f1: 0.6187 - val_recall: 0.6720 - val_precision: 0.5747\n",
      "\n",
      "Epoch 00036: val_f1 did not improve\n",
      "Epoch 37/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1087 - f1: 0.5837 - recall: 0.6239 - precision: 0.5487 - val_loss: 1.1464 - val_f1: 0.6001 - val_recall: 0.6647 - val_precision: 0.5484\n",
      "\n",
      "Epoch 00037: val_f1 did not improve\n",
      "Epoch 38/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1085 - f1: 0.5826 - recall: 0.6354 - precision: 0.5385 - val_loss: 1.1565 - val_f1: 0.5982 - val_recall: 0.6493 - val_precision: 0.5566\n",
      "\n",
      "Epoch 00038: val_f1 did not improve\n",
      "Epoch 39/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1075 - f1: 0.5837 - recall: 0.6286 - precision: 0.5454 - val_loss: 1.1083 - val_f1: 0.6152 - val_recall: 0.6700 - val_precision: 0.5700\n",
      "\n",
      "Epoch 00039: val_f1 did not improve\n",
      "Epoch 40/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1059 - f1: 0.5875 - recall: 0.6380 - precision: 0.5448 - val_loss: 1.1308 - val_f1: 0.6066 - val_recall: 0.6773 - val_precision: 0.5502\n",
      "\n",
      "Epoch 00040: val_f1 did not improve\n",
      "Epoch 41/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1030 - f1: 0.5903 - recall: 0.6380 - precision: 0.5497 - val_loss: 1.1192 - val_f1: 0.6166 - val_recall: 0.6686 - val_precision: 0.5734\n",
      "\n",
      "Epoch 00041: val_f1 did not improve\n",
      "Epoch 42/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1009 - f1: 0.5842 - recall: 0.6392 - precision: 0.5382 - val_loss: 1.1300 - val_f1: 0.6042 - val_recall: 0.6768 - val_precision: 0.5468\n",
      "\n",
      "Epoch 00042: val_f1 did not improve\n",
      "Epoch 43/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1090 - f1: 0.5821 - recall: 0.6285 - precision: 0.5429 - val_loss: 1.0726 - val_f1: 0.6314 - val_recall: 0.6864 - val_precision: 0.5853\n",
      "\n",
      "Epoch 00043: val_f1 did not improve\n",
      "Epoch 44/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1079 - f1: 0.5863 - recall: 0.6386 - precision: 0.5424 - val_loss: 1.0789 - val_f1: 0.6262 - val_recall: 0.6845 - val_precision: 0.5780\n",
      "\n",
      "Epoch 00044: val_f1 did not improve\n",
      "Epoch 45/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1026 - f1: 0.5859 - recall: 0.6408 - precision: 0.5402 - val_loss: 1.1052 - val_f1: 0.6172 - val_recall: 0.6643 - val_precision: 0.5778\n",
      "\n",
      "Epoch 00045: val_f1 did not improve\n",
      "Epoch 46/100\n",
      "8291/8291 [==============================] - 15s 2ms/step - loss: 1.1003 - f1: 0.5873 - recall: 0.6337 - precision: 0.5480 - val_loss: 1.0909 - val_f1: 0.6318 - val_recall: 0.6758 - val_precision: 0.5944\n",
      "\n",
      "Epoch 00046: val_f1 did not improve\n",
      "Epoch 47/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.1038 - f1: 0.5874 - recall: 0.6343 - precision: 0.5475 - val_loss: 1.1047 - val_f1: 0.6230 - val_recall: 0.6942 - val_precision: 0.5668\n",
      "\n",
      "Epoch 00047: val_f1 did not improve\n",
      "Epoch 48/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.0920 - f1: 0.5911 - recall: 0.6501 - precision: 0.5425 - val_loss: 1.0830 - val_f1: 0.6203 - val_recall: 0.6623 - val_precision: 0.5844\n",
      "\n",
      "Epoch 00048: val_f1 did not improve\n",
      "Epoch 49/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.0962 - f1: 0.5861 - recall: 0.6418 - precision: 0.5400 - val_loss: 1.1025 - val_f1: 0.6104 - val_recall: 0.6691 - val_precision: 0.5623\n",
      "\n",
      "Epoch 00049: val_f1 did not improve\n",
      "Epoch 50/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.0970 - f1: 0.5859 - recall: 0.6349 - precision: 0.5445 - val_loss: 1.0908 - val_f1: 0.6227 - val_recall: 0.6782 - val_precision: 0.5763\n",
      "\n",
      "Epoch 00050: val_f1 did not improve\n",
      "Epoch 51/100\n",
      "8291/8291 [==============================] - 15s 2ms/step - loss: 1.0959 - f1: 0.5879 - recall: 0.6432 - precision: 0.5421 - val_loss: 1.1020 - val_f1: 0.6160 - val_recall: 0.6778 - val_precision: 0.5657\n",
      "\n",
      "Epoch 00051: val_f1 did not improve\n",
      "Epoch 52/100\n",
      "8291/8291 [==============================] - 15s 2ms/step - loss: 1.0943 - f1: 0.5864 - recall: 0.6509 - precision: 0.5341 - val_loss: 1.1134 - val_f1: 0.6077 - val_recall: 0.6705 - val_precision: 0.5572\n",
      "\n",
      "Epoch 00052: val_f1 did not improve\n",
      "Epoch 53/100\n",
      "8291/8291 [==============================] - 15s 2ms/step - loss: 1.0911 - f1: 0.5907 - recall: 0.6518 - precision: 0.5407 - val_loss: 1.0974 - val_f1: 0.6083 - val_recall: 0.6753 - val_precision: 0.5548\n",
      "\n",
      "Epoch 00053: val_f1 did not improve\n",
      "Epoch 54/100\n",
      "8291/8291 [==============================] - 16s 2ms/step - loss: 1.0874 - f1: 0.5901 - recall: 0.6467 - precision: 0.5431 - val_loss: 1.1249 - val_f1: 0.6124 - val_recall: 0.6623 - val_precision: 0.5705\n",
      "\n",
      "Epoch 00054: val_f1 did not improve\n",
      "Epoch 55/100\n",
      "8291/8291 [==============================] - 15s 2ms/step - loss: 1.0899 - f1: 0.5918 - recall: 0.6501 - precision: 0.5436 - val_loss: 1.1191 - val_f1: 0.6101 - val_recall: 0.6700 - val_precision: 0.5612\n",
      "\n",
      "Epoch 00055: val_f1 did not improve\n"
     ]
    }
   ],
   "source": [
    "keras_m = train(X_train_idf, y_train_ohe, epochs=100, name='wieghts/tfidf')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:50:49.163669Z",
     "start_time": "2018-04-28T18:50:49.004030Z"
    },
    "scrolled": true
   },
   "source": [
    "keras_m = train(X_train_idf, y_train, epochs=1, name='tfidf')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T12:27:51.929602Z",
     "start_time": "2018-04-26T11:10:02.597223Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "### unfortunately, bad score\n",
    "train(X_train_tf, y_train, name='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T19:15:18.628210Z",
     "start_time": "2018-04-28T19:15:17.932355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "922/922 [==============================] - 1s 748us/step\n",
      "F-score: 0.5913594516394195\n"
     ]
    }
   ],
   "source": [
    "print ('F-score:',keras_m.evaluate(X_valid_idf.toarray(), y_valid_ohe)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison with Xgboost score a little worse. I make it like this. I'll try to make more solution with keras in another notebooks. We can try:\n",
    "\n",
    "- KerasClassifier\n",
    "- Text vectorizing with keras preprocessing tools\n",
    "- More data to NN - better score, that's why XGBoost perform better.\n",
    "- Add more layers!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Content",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
