{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Content<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load\" data-toc-modified-id=\"Load-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load</a></span></li><li><span><a href=\"#Tokenizing\" data-toc-modified-id=\"Tokenizing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Tokenizing</a></span></li><li><span><a href=\"#Building-model\" data-toc-modified-id=\"Building-model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Building model</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:22:02.901991Z",
     "start_time": "2018-04-28T18:22:02.620787Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:22:02.985108Z",
     "start_time": "2018-04-28T18:22:02.903485Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv', index_col='time')\n",
    "test = pd.read_csv('data/test.csv', index_col='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:22:02.990718Z",
     "start_time": "2018-04-28T18:22:02.986567Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid = train[[col for col in train.columns if col!='severity']], train['severity'],\\\n",
    "                                     test[[col for col in train.columns if col!='severity']], test['severity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:22:04.120467Z",
     "start_time": "2018-04-28T18:22:02.992527Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denis/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPool1D, Dropout, concatenate\n",
    "from keras.preprocessing import text as keras_text, sequence as keras_seq\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:22:04.123627Z",
     "start_time": "2018-04-28T18:22:04.121926Z"
    }
   },
   "outputs": [],
   "source": [
    "# define network parameters\n",
    "max_features = 100\n",
    "maxlen = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:22:06.614181Z",
     "start_time": "2018-04-28T18:22:04.124969Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = keras_text.Tokenizer(num_words=10000, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\n",
    "                                 lower=True, split=' ', char_level=True, oov_token=None)\n",
    "\n",
    "tokenizer.fit_on_texts(list(X_train['message_encoding']))\n",
    "# train data\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(X_train['message_encoding'])\n",
    "X_t = keras_seq.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "\n",
    "# test data\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(X_valid['message_encoding'])\n",
    "X_te = keras_seq.pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:22:06.622168Z",
     "start_time": "2018-04-28T18:22:06.616700Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:22:06.648976Z",
     "start_time": "2018-04-28T18:22:06.623584Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(conv_layers = 1, \n",
    "                dilation_rates = [2, 4], \n",
    "                embed_size = 32):\n",
    "    \n",
    "    inp = Input(shape=(None, ))\n",
    "    x = Embedding(input_dim = len(tokenizer.word_counts)+1, \n",
    "                  output_dim = embed_size)(inp)\n",
    "    prefilt_x = Dropout(0.25)(x)\n",
    "    out_conv = []\n",
    "    # dilation rate lets us use ngrams and skip grams to process \n",
    "    for dilation_rate in dilation_rates:\n",
    "        x = prefilt_x\n",
    "        for i in range(2):\n",
    "            if dilation_rate>0:\n",
    "                x = Conv1D(16*2**(i), \n",
    "                           kernel_size = 3, \n",
    "                           dilation_rate = dilation_rate,\n",
    "                          activation = 'relu',\n",
    "                          name = 'ngram_{}_cnn_{}'.format(dilation_rate, i)\n",
    "                          )(x)\n",
    "            else:\n",
    "                x = Conv1D(16*2**(i), \n",
    "                           kernel_size = 1,\n",
    "                          activation = 'relu',\n",
    "                          name = 'word_fcl_{}'.format(i))(x)\n",
    "        out_conv += [Dropout(0.25)(GlobalMaxPool1D()(x))]\n",
    "    x = concatenate(out_conv, axis = -1)    \n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='kullback_leibler_divergence',\n",
    "                  optimizer='adam',\n",
    "                  metrics=[precision, recall, f1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:22:07.017023Z",
     "start_time": "2018-04-28T18:22:06.650360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 32)     5152        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 32)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ngram_2_cnn_0 (Conv1D)          (None, None, 16)     1552        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ngram_4_cnn_0 (Conv1D)          (None, None, 16)     1552        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ngram_2_cnn_1 (Conv1D)          (None, None, 32)     1568        ngram_2_cnn_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ngram_4_cnn_1 (Conv1D)          (None, None, 32)     1568        ngram_4_cnn_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 32)           0           ngram_2_cnn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 32)           0           ngram_4_cnn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32)           0           global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64)           0           dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           4160        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           2080        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            33          dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 17,665\n",
      "Trainable params: 17,665\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Unfortunately, the memory in my GPU is not enough to apply one-hot to target, so try to train the model on all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:22:07.022073Z",
     "start_time": "2018-04-28T18:22:07.019030Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(X, y, batch_size = 16, epochs = 100, name='Test'):\n",
    "    file_path= name + \"best_weights.h5\"\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_f1', \n",
    "                                 verbose=1, save_best_only=True, mode='max')\n",
    "        \n",
    "    early = EarlyStopping(monitor=\"val_f1\", mode=\"max\", patience=30)\n",
    "        \n",
    "    callbacks_list = [checkpoint, early]\n",
    "    model.fit(X, y,\n",
    "              validation_split=0.2,\n",
    "              batch_size=batch_size, \n",
    "              epochs=epochs, \n",
    "              callbacks=callbacks_list)\n",
    "    model.load_weights(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:23:10.067875Z",
     "start_time": "2018-04-28T18:22:07.024406Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8291 samples, validate on 2073 samples\n",
      "Epoch 1/100\n",
      "8291/8291 [==============================] - 3s 415us/step - loss: 0.0245 - precision: 0.9569 - recall: 0.9999 - f1: 0.9768 - val_loss: 4.8116e-07 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.97041, saving model to wieghts/secondbest_weights.h5\n",
      "Epoch 2/100\n",
      "8291/8291 [==============================] - 2s 232us/step - loss: 1.1035e-05 - precision: 0.9539 - recall: 1.0000 - f1: 0.9756 - val_loss: -4.4446e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00002: val_f1 did not improve\n",
      "Epoch 3/100\n",
      "8291/8291 [==============================] - 2s 234us/step - loss: 3.3480e-06 - precision: 0.9539 - recall: 1.0000 - f1: 0.9756 - val_loss: -8.9128e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00003: val_f1 did not improve\n",
      "Epoch 4/100\n",
      "8291/8291 [==============================] - 2s 238us/step - loss: 1.8439e-06 - precision: 0.9539 - recall: 1.0000 - f1: 0.9757 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00004: val_f1 did not improve\n",
      "Epoch 5/100\n",
      "8291/8291 [==============================] - 2s 241us/step - loss: 6.0464e-07 - precision: 0.9539 - recall: 1.0000 - f1: 0.9757 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00005: val_f1 did not improve\n",
      "Epoch 6/100\n",
      "8291/8291 [==============================] - 2s 241us/step - loss: 3.0771e-07 - precision: 0.9539 - recall: 1.0000 - f1: 0.9756 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00006: val_f1 did not improve\n",
      "Epoch 7/100\n",
      "8291/8291 [==============================] - 2s 247us/step - loss: 5.4820e-07 - precision: 0.9539 - recall: 1.0000 - f1: 0.9757 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00007: val_f1 did not improve\n",
      "Epoch 8/100\n",
      "8291/8291 [==============================] - 2s 248us/step - loss: 1.3957e-07 - precision: 0.9539 - recall: 1.0000 - f1: 0.9757 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00008: val_f1 did not improve\n",
      "Epoch 9/100\n",
      "8291/8291 [==============================] - 2s 242us/step - loss: 7.5623e-09 - precision: 0.9539 - recall: 1.0000 - f1: 0.9756 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00009: val_f1 did not improve\n",
      "Epoch 10/100\n",
      "8291/8291 [==============================] - 2s 245us/step - loss: -4.5440e-09 - precision: 0.9539 - recall: 1.0000 - f1: 0.9757 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00010: val_f1 did not improve\n",
      "Epoch 11/100\n",
      "8291/8291 [==============================] - 2s 236us/step - loss: 1.9810e-08 - precision: 0.9539 - recall: 1.0000 - f1: 0.9757 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00011: val_f1 did not improve\n",
      "Epoch 12/100\n",
      "8291/8291 [==============================] - 2s 245us/step - loss: 5.0191e-08 - precision: 0.9539 - recall: 1.0000 - f1: 0.9756 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00012: val_f1 did not improve\n",
      "Epoch 13/100\n",
      "8291/8291 [==============================] - 2s 263us/step - loss: 7.7878e-08 - precision: 0.9539 - recall: 1.0000 - f1: 0.9757 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00013: val_f1 did not improve\n",
      "Epoch 14/100\n",
      "8291/8291 [==============================] - 2s 257us/step - loss: -4.5853e-08 - precision: 0.9539 - recall: 1.0000 - f1: 0.9757 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00014: val_f1 did not improve\n",
      "Epoch 15/100\n",
      "8291/8291 [==============================] - 2s 238us/step - loss: -6.2703e-08 - precision: 0.9539 - recall: 1.0000 - f1: 0.9757 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00015: val_f1 did not improve\n",
      "Epoch 16/100\n",
      "8291/8291 [==============================] - 2s 234us/step - loss: -5.7584e-08 - precision: 0.9539 - recall: 1.0000 - f1: 0.9757 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00016: val_f1 did not improve\n",
      "Epoch 17/100\n",
      "8291/8291 [==============================] - 2s 240us/step - loss: -6.4313e-08 - precision: 0.9539 - recall: 1.0000 - f1: 0.9757 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00017: val_f1 did not improve\n",
      "Epoch 18/100\n",
      "8291/8291 [==============================] - 2s 245us/step - loss: -6.7059e-08 - precision: 0.9539 - recall: 1.0000 - f1: 0.9757 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00018: val_f1 did not improve\n",
      "Epoch 19/100\n",
      "8291/8291 [==============================] - 2s 252us/step - loss: -7.1258e-08 - precision: 0.9539 - recall: 1.0000 - f1: 0.9757 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00019: val_f1 did not improve\n",
      "Epoch 20/100\n",
      "8291/8291 [==============================] - 2s 230us/step - loss: -5.8706e-08 - precision: 0.9539 - recall: 1.0000 - f1: 0.9757 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00020: val_f1 did not improve\n",
      "Epoch 21/100\n",
      "8291/8291 [==============================] - 2s 231us/step - loss: -7.2508e-08 - precision: 0.9539 - recall: 1.0000 - f1: 0.9757 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00021: val_f1 did not improve\n",
      "Epoch 22/100\n",
      "8291/8291 [==============================] - 2s 228us/step - loss: -7.2868e-08 - precision: 0.9539 - recall: 1.0000 - f1: 0.9758 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00022: val_f1 did not improve\n",
      "Epoch 23/100\n",
      "8291/8291 [==============================] - 2s 223us/step - loss: -6.2674e-08 - precision: 0.9539 - recall: 1.0000 - f1: 0.9757 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00023: val_f1 did not improve\n",
      "Epoch 24/100\n",
      "8291/8291 [==============================] - 2s 229us/step - loss: -7.1473e-08 - precision: 0.9539 - recall: 1.0000 - f1: 0.9757 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00024: val_f1 did not improve\n",
      "Epoch 25/100\n",
      "8291/8291 [==============================] - 2s 229us/step - loss: -6.0115e-08 - precision: 0.9539 - recall: 1.0000 - f1: 0.9756 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00025: val_f1 did not improve\n",
      "Epoch 26/100\n",
      "8291/8291 [==============================] - 2s 225us/step - loss: -7.3716e-08 - precision: 0.9539 - recall: 1.0000 - f1: 0.9757 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00026: val_f1 did not improve\n",
      "Epoch 27/100\n",
      "8291/8291 [==============================] - 2s 228us/step - loss: -7.2710e-08 - precision: 0.9539 - recall: 1.0000 - f1: 0.9757 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00027: val_f1 did not improve\n",
      "Epoch 28/100\n",
      "8291/8291 [==============================] - 2s 228us/step - loss: -7.3846e-08 - precision: 0.9539 - recall: 1.0000 - f1: 0.9757 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00028: val_f1 did not improve\n",
      "Epoch 29/100\n",
      "8291/8291 [==============================] - 2s 227us/step - loss: -7.2854e-08 - precision: 0.9539 - recall: 1.0000 - f1: 0.9757 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00029: val_f1 did not improve\n",
      "Epoch 30/100\n",
      "8291/8291 [==============================] - 2s 233us/step - loss: -7.3443e-08 - precision: 0.9539 - recall: 1.0000 - f1: 0.9756 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n",
      "\n",
      "Epoch 00030: val_f1 did not improve\n",
      "Epoch 31/100\n",
      "8291/8291 [==============================] - 2s 225us/step - loss: -7.4234e-08 - precision: 0.9539 - recall: 1.0000 - f1: 0.9757 - val_loss: -8.9415e-08 - val_precision: 0.9445 - val_recall: 1.0000 - val_f1: 0.9704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00031: val_f1 did not improve\n"
     ]
    }
   ],
   "source": [
    "keras_m = train(X_t, y_train, epochs=100, name='wieghts/second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T18:23:10.171632Z",
     "start_time": "2018-04-28T18:23:10.069218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "922/922 [==============================] - 0s 104us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.7640592121629663e-07, 0.93058568329718, 1.0, 0.9629979288759045]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_m.evaluate(X_te,y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems that we have the best score. F1-score - 0.9629979288759045, but I don't believe in it. Let's try another approach in another notebook. I also tried one-hot encoding for target variable, but kernel dies, because of memory limitation in GPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Content",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
