{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Content<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load\" data-toc-modified-id=\"Load-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load</a></span></li><li><span><a href=\"#Calculate-Term-Frequencies\" data-toc-modified-id=\"Calculate-Term-Frequencies-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Calculate Term Frequencies</a></span></li><li><span><a href=\"#Build-keras-model\" data-toc-modified-id=\"Build-keras-model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Build keras model</a></span></li><li><span><a href=\"#Train-and-save-our-model\" data-toc-modified-id=\"Train-and-save-our-model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Train and save our model</a></span></li><li><span><a href=\"#Testing-on-validation-set\" data-toc-modified-id=\"Testing-on-validation-set-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Testing on validation set</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T22:46:05.218589Z",
     "start_time": "2018-04-27T22:46:05.054947Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T22:46:05.290408Z",
     "start_time": "2018-04-27T22:46:05.220113Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv', index_col='time')\n",
    "test = pd.read_csv('data/test.csv', index_col='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T22:46:05.295620Z",
     "start_time": "2018-04-27T22:46:05.291900Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid = train[[col for col in train.columns if col!='severity']], train['severity'],\\\n",
    "                                     test[[col for col in train.columns if col!='severity']], test['severity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Calculate Term Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T22:46:05.549741Z",
     "start_time": "2018-04-27T22:46:05.296981Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "\n",
    "import scipy.sparse as sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T22:46:05.554023Z",
     "start_time": "2018-04-27T22:46:05.551002Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Vect(df, use_idf=True, min_df=1, max_df=1.0, ngram_range = (4,8)):\n",
    "    '''\n",
    "    Tfidf or tf vectorizing\n",
    "    \n",
    "    us_idf: bool([True, False]), use tfidf or not;\n",
    "    min_df: float()\n",
    "    max_df: float()\n",
    "    ngram_range: tuple(None, None)\n",
    "    '''\n",
    "    if use_idf == True:\n",
    "        tfidf = TfidfVectorizer(stop_words=stopwords.words('english'), preprocessor=None,\n",
    "                                ngram_range=ngram_range, strip_accents='ascii', analyzer='word',\n",
    "                                min_df = min_df, max_df=max_df, tokenizer=None)\n",
    "    \n",
    "        message_encoding = tfidf.fit_transform(df['message_encoding'])\n",
    "        \n",
    "        return message_encoding, tfidf\n",
    "    \n",
    "    else:\n",
    "        tf = CountVectorizer(stop_words=stopwords.words('english'), preprocessor=None,\n",
    "                             ngram_range=ngram_range, strip_accents='ascii', analyzer='word',\n",
    "                             min_df = min_df, max_df=max_df, tokenizer=None)\n",
    "\n",
    "        message_encoding = tf.fit_transform(df['message_encoding'])\n",
    "        \n",
    "        return message_encoding, tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T22:46:34.062002Z",
     "start_time": "2018-04-27T22:46:05.555345Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_idf, idf_m = Vect(X_train, max_df=0.80, min_df=30)\n",
    "X_valid_idf = idf_m.transform(X_valid['message_encoding'])\n",
    "\n",
    "X_train_tf, tf_m = Vect(X_train, use_idf=False, max_df=0.80, min_df=30)\n",
    "X_valid_tf = tf_m.transform(X_valid['message_encoding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T22:46:34.080322Z",
     "start_time": "2018-04-27T22:46:34.063352Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_idf = sps.hstack((X_train_idf, train[['author_name', 'author_email', 'committer_name', \n",
    "                                              'committer_email', 'tm_year', 'tm_mon', 'tm_mday', \n",
    "                                              'tm_hour', 'tm_wday', 'tm_yday', 'first_w']]))\n",
    "\n",
    "X_valid_idf = sps.hstack((X_valid_idf, test[['author_name', 'author_email', 'committer_name', \n",
    "                                             'committer_email', 'tm_year', 'tm_mon', 'tm_mday', \n",
    "                                             'tm_hour', 'tm_wday', 'tm_yday', 'first_w']]))\n",
    "\n",
    "X_train_tf = sps.hstack((X_train_tf, train[['author_name', 'author_email', 'committer_name', \n",
    "                                              'committer_email', 'tm_year', 'tm_mon', 'tm_mday', \n",
    "                                              'tm_hour', 'tm_wday', 'tm_yday', 'first_w']]))\n",
    "\n",
    "X_valid_tf = sps.hstack((X_valid_tf, test[['author_name', 'author_email', 'committer_name', \n",
    "                                           'committer_email', 'tm_year', 'tm_mon', 'tm_mday', \n",
    "                                           'tm_hour', 'tm_wday', 'tm_yday', 'first_w']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Build keras model\n",
    "\n",
    "I chose the most popular architecture and adjust it a little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T22:46:35.344326Z",
     "start_time": "2018-04-27T22:46:34.081556Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denis/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout\n",
    "from keras.layers import LSTM, Activation\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T22:46:35.347633Z",
     "start_time": "2018-04-27T22:46:35.345840Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Embedding\n",
    "max_features = 500\n",
    "maxlen = 3652\n",
    "embedding_size = 32\n",
    "\n",
    "# Convolution\n",
    "filter_length = 4\n",
    "nb_filter = 32\n",
    "pool_length = 8\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T22:46:35.372209Z",
     "start_time": "2018-04-27T22:46:35.348844Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T22:46:35.591692Z",
     "start_time": "2018-04-27T22:46:35.373251Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denis/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=32, kernel_size=4, strides=1, padding=\"valid\")`\n",
      "  \n",
      "/home/denis/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=8)`\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "model.add(MaxPooling1D(pool_length=pool_length))\n",
    "model.add(LSTM(lstm_output_size))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[f1, recall, precision])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T22:46:35.595989Z",
     "start_time": "2018-04-27T22:46:35.593434Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 3652, 32)          16000     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3652, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 3649, 32)          4128      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 456, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 150)               109800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 755       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 130,683\n",
      "Trainable params: 130,683\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T22:46:35.609982Z",
     "start_time": "2018-04-27T22:46:35.598058Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train_ohe = to_categorical(y_train)\n",
    "y_valid_ohe = to_categorical(y_valid)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T22:01:06.170314Z",
     "start_time": "2018-04-27T22:01:06.161072Z"
    },
    "scrolled": true
   },
   "source": [
    "def train(X, y, batch_size = 32, epochs = 10, name='Test'):\n",
    "    models = []\n",
    "    for i in [0,1,2,3,4]:\n",
    "        print ('Testing y == %d' % i)\n",
    "        file_path= name + \"best_weights_%d.h5\" % i\n",
    "        \n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_f1', \n",
    "                                     verbose=1, save_best_only=True, mode='max')\n",
    "        \n",
    "        early = EarlyStopping(monitor=\"val_f1\", mode=\"max\", patience=20)\n",
    "        \n",
    "        callbacks_list = [checkpoint, early]\n",
    "        model.fit(X.toarray(), y==i,\n",
    "                  validation_split=0.2,\n",
    "                  batch_size=batch_size, \n",
    "                  epochs=epochs, \n",
    "                  callbacks=callbacks_list)\n",
    "        models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T22:46:35.620880Z",
     "start_time": "2018-04-27T22:46:35.611300Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(X, y, batch_size = 64, epochs = 100, name='Test'):\n",
    "    file_path= name + \"best_weights.h5\"\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_f1', \n",
    "                                 verbose=1, save_best_only=True, mode='max')\n",
    "        \n",
    "    early = EarlyStopping(monitor=\"val_f1\", mode=\"max\", patience=30)\n",
    "        \n",
    "    callbacks_list = [checkpoint, early]\n",
    "    model.fit(X.toarray(), y,\n",
    "              validation_split=0.2,\n",
    "              batch_size=batch_size, \n",
    "              epochs=epochs, \n",
    "              callbacks=callbacks_list)\n",
    "    model.load_weights(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T22:52:12.998786Z",
     "start_time": "2018-04-27T22:46:35.622166Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8291 samples, validate on 2073 samples\n",
      "Epoch 1/100\n",
      "8291/8291 [==============================] - 43s 5ms/step - loss: 1.2902 - f1: 0.5309 - recall: 0.5610 - precision: 0.5163 - val_loss: 1.1320 - val_f1: 0.5644 - val_recall: 0.8466 - val_precision: 0.4233\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.56440, saving model to wieghts/tfidfbest_weights.h5\n",
      "Epoch 2/100\n",
      "8291/8291 [==============================] - 42s 5ms/step - loss: 1.2409 - f1: 0.5483 - recall: 0.6076 - precision: 0.5068 - val_loss: 1.0929 - val_f1: 0.6322 - val_recall: 0.7062 - val_precision: 0.5744\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.56440 to 0.63220, saving model to wieghts/tfidfbest_weights.h5\n",
      "Epoch 3/100\n",
      "8291/8291 [==============================] - 42s 5ms/step - loss: 1.1906 - f1: 0.5609 - recall: 0.6534 - precision: 0.4921 - val_loss: 1.0894 - val_f1: 0.6249 - val_recall: 0.7405 - val_precision: 0.5423\n",
      "\n",
      "Epoch 00003: val_f1 did not improve\n",
      "Epoch 4/100\n",
      "8291/8291 [==============================] - 41s 5ms/step - loss: 1.1669 - f1: 0.5659 - recall: 0.6494 - precision: 0.5030 - val_loss: 1.0708 - val_f1: 0.6415 - val_recall: 0.6918 - val_precision: 0.5995\n",
      "\n",
      "Epoch 00004: val_f1 improved from 0.63220 to 0.64149, saving model to wieghts/tfidfbest_weights.h5\n",
      "Epoch 5/100\n",
      "8291/8291 [==============================] - 42s 5ms/step - loss: 1.1475 - f1: 0.5770 - recall: 0.6333 - precision: 0.5308 - val_loss: 1.0590 - val_f1: 0.6440 - val_recall: 0.6614 - val_precision: 0.6279\n",
      "\n",
      "Epoch 00005: val_f1 improved from 0.64149 to 0.64405, saving model to wieghts/tfidfbest_weights.h5\n",
      "Epoch 6/100\n",
      "8291/8291 [==============================] - 42s 5ms/step - loss: 1.1309 - f1: 0.5840 - recall: 0.6336 - precision: 0.5425 - val_loss: 1.0796 - val_f1: 0.6383 - val_recall: 0.6618 - val_precision: 0.6169\n",
      "\n",
      "Epoch 00006: val_f1 did not improve\n",
      "Epoch 7/100\n",
      "8291/8291 [==============================] - 42s 5ms/step - loss: 1.1199 - f1: 0.5845 - recall: 0.6329 - precision: 0.5435 - val_loss: 1.0707 - val_f1: 0.6333 - val_recall: 0.6773 - val_precision: 0.5956\n",
      "\n",
      "Epoch 00007: val_f1 did not improve\n",
      "Epoch 8/100\n",
      "8291/8291 [==============================] - 42s 5ms/step - loss: 1.1042 - f1: 0.5863 - recall: 0.6309 - precision: 0.5479 - val_loss: 1.0815 - val_f1: 0.6326 - val_recall: 0.6792 - val_precision: 0.5930\n",
      "\n",
      "Epoch 00008: val_f1 did not improve\n"
     ]
    }
   ],
   "source": [
    "keras_m = train(X_train_idf, y_train_ohe, epochs=100, name='wieghts/tfidf')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T22:01:06.758727Z",
     "start_time": "2018-04-27T22:01:06.171924Z"
    },
    "scrolled": true
   },
   "source": [
    "keras_m = train(X_train_idf, y_train, epochs=1, name='tfidf')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-26T12:27:51.929602Z",
     "start_time": "2018-04-26T11:10:02.597223Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "### unfortunately, bad score\n",
    "train(X_train_tf, y_train, name='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Testing on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T22:52:14.774560Z",
     "start_time": "2018-04-27T22:52:13.000107Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "922/922 [==============================] - 2s 2ms/step\n",
      "F-score: 0.6066880445935462\n"
     ]
    }
   ],
   "source": [
    "print ('F-score:',keras_m.evaluate(X_valid_idf.toarray(), y_valid_ohe)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison with Xgboost score a little worse. I make it like this. I'll try to make more solution with keras in another notebooks. We can try:\n",
    "\n",
    "- KerasClassifier\n",
    "- Text vectorizing with keras preprocessing tools\n",
    "- More data to NN - better score, that's why XGBoost perform better.\n",
    "- Add more layers!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Content",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
