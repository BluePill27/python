{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = pd.read_csv('features/features.csv', index_col='match_id')\n",
    "features.drop('duration radiant_win tower_status_radiant tower_status_dire barracks_status_radiant barracks_status_dire'.split(' '), inplace=True, axis=1)\n",
    "\n",
    "features_test = pd.read_csv('features_test/features_test.csv', index_col='match_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , Y_train = features.fillna(0).as_matrix().astype(float), pd.read_csv('features/features.csv', index_col='match_id')['radiant_win'].fillna(0).as_matrix().astype(float)\n",
    "X_test = features_test.fillna(0).as_matrix().astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Оцените качество логистической регрессии (sklearn.linear_model.LogisticRegression с L2-регуляризацией) с помощью кросс-валидации по той же схеме, которая использовалась для градиентного бустинга. Подберите при этом лучший параметр регуляризации (C). Какое наилучшее качество у вас получилось? Как оно соотносится с качеством градиентного бустинга? Чем вы можете объяснить эту разницу? Быстрее ли работает логистическая регрессия по сравнению с градиентным бустингом?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_grid(X_train, Y_train, c=[10.0 ** i for i in range(-9, 8)]):\n",
    "    X_train = scale(X_train)\n",
    "    \n",
    "    grid = {'C': c}\n",
    "    clf = LogisticRegression(penalty='l2')\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    gs = GridSearchCV(clf, scoring='roc_auc', param_grid=grid, cv=cv, return_train_score=True)\n",
    "    gs.fit(X_train,Y_train)\n",
    "    print ('Time elapsed:', datetime.datetime.now() - start_time)\n",
    "    \n",
    "    means = gs.cv_results_['mean_test_score']\n",
    "    stds = gs.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, gs.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    return c[np.argmax(means)]\n",
    "    \n",
    "def testing_cros(X_train, Y_train, c=[10.0 ** i for i in range(-9, 8)]):     \n",
    "    start_time = datetime.datetime.now()\n",
    "    scores=[]\n",
    "    X_train = scale(X_train)\n",
    "    for x in c:\n",
    "        print (x)\n",
    "        scores.append(np.array(cross_val_score(LogisticRegression(C=x), X_train, Y_train, scoring='roc_auc', cv=cv)))\n",
    "    scores=np.array(scores)\n",
    "    print ('Time elapsed:', datetime.datetime.now() - start_time)\n",
    "    print (c[np.argmax([np.mean(sc) for sc in scores])])\n",
    "    return c[np.argmax([np.mean(sc) for sc in scores])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Denis\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0:02:47.643152\n",
      "0.687 (+/-0.003) for {'C': 1e-09}\n",
      "0.687 (+/-0.003) for {'C': 1e-08}\n",
      "0.687 (+/-0.003) for {'C': 1e-07}\n",
      "0.688 (+/-0.003) for {'C': 1e-06}\n",
      "0.695 (+/-0.003) for {'C': 1e-05}\n",
      "0.711 (+/-0.003) for {'C': 0.0001}\n",
      "0.716 (+/-0.003) for {'C': 0.001}\n",
      "0.717 (+/-0.002) for {'C': 0.01}\n",
      "0.717 (+/-0.002) for {'C': 0.1}\n",
      "0.717 (+/-0.002) for {'C': 1.0}\n",
      "0.717 (+/-0.002) for {'C': 10.0}\n",
      "0.717 (+/-0.002) for {'C': 100.0}\n",
      "0.717 (+/-0.002) for {'C': 1000.0}\n",
      "0.717 (+/-0.002) for {'C': 10000.0}\n",
      "0.717 (+/-0.002) for {'C': 100000.0}\n",
      "0.717 (+/-0.002) for {'C': 1000000.0}\n",
      "0.717 (+/-0.002) for {'C': 10000000.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_grid(scale(X_train), Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
