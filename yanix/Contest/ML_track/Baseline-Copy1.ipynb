{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yandex.Algorithm ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый из файлов субтитров в датасете OpenSubtitles [2], который мы использовали в качестве источника реплик и разговоров, содержит упорядоченный набор реплик. В большинстве случаев, каждая реплика – это ответ на предыдущую, в разговоре между двумя персонажами фильма. Мы случайно выбрали эпизоды этих разговоров в качестве наших тренировочных и тестовых примеров.\n",
    "\n",
    "Каждый эпизод состоит из двух частей – контекста (Context) и финальной реплики (Reply). Например,\n",
    "\n",
    "- context_2: Персонаж A говорит реплику \n",
    "\n",
    "- context_1: Персонаж B отвечает на нее \n",
    "\n",
    "- context_0: Персонаж А произносит вторую реплику \n",
    "\n",
    "reply: Персонаж B отвечает на вторую реплику \n",
    "Контекстная часть может состоять из трех реплик (как в примере) – в 50% случаев, двух – в 25%, и одного – в оставшихся 25% случаев. Финальная реплика (Reply) всегда завершает любой эпизод, то есть следует за контекстом (Context). Задача участников – найти наиболее подходящую и интересную реплику для данного контекста среди предложенных кандидатов (числом до 6), случайно выбранных из топа кандидатов, возвращенных бейзлайном высокого качества, натренированным командой Алисы (который, в свою очередь, отобрал кандидатов среди всех возможных реплик OpenSubtitles).\n",
    "\n",
    "Все реплики-кандидаты размечены асессорами на сервисе Яндекс.Толока с помощью следующей инструкции для разметки:\n",
    "\n",
    "- Good (2): реплика уместна (имеет смысл для данного контекста) и интересна (нетривиальна, специфична именно для данного контекста, мотивирует продолжать разговор)\n",
    "\n",
    "- Neutral (1): реплика уместна (имеет смысл для данного контекста), но не интересна (тривиальна, не специфична для данного контекста и скорее подталкивает пользователя закончить разговор)\n",
    "\n",
    "- Bad (0): реплика не имеет никакого смысла в данном контексте\n",
    "\n",
    "Каждая метка в тренировочной части датасета (и только в ней), сопровождается также уверенностью (confidence) – числом в интервале от 0 до 1 – которое показывает насколько уверенными в своей разметке были асессоры с Толоки, совместно предложившие данную метку. Мы хотим обратить особое внимание участников на эту информацию, она может быть очень полезна при обучении их моделей.\n",
    "\n",
    "Мы хотим особо отметить, что все участники имеют право скачать датасет OpenSubtitles [2], который использовался для подготовки датасета и применять его для тренировки своих моделей по своему усмотрению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T12:36:45.973220Z",
     "start_time": "2018-04-14T12:36:45.770991Z"
    },
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T20:08:49.456254Z",
     "start_time": "2018-04-12T20:08:49.346371Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- context_id – идентификатор эпизода\n",
    "- context_2,context_1,context_0 – текст реплик, предшествующих финальной (может состоять из трех частей)\n",
    "- reply_id – идентификатор реплики-кандидата\n",
    "- reply – текст реплики-кандидата\n",
    "- label – метка реплики-кандидата (good, neutral или bad)\n",
    "- confidence - уверенность в метке реплики-кандидата (число от 0 до 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T12:36:46.198075Z",
     "start_time": "2018-04-14T12:36:46.013912Z"
    },
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.tsv', sep='\\t', quotechar=' ', header = None)\n",
    "df.columns = ['context_id', 'context_2', 'context_1', 'context_0', 'reply_id', 'reply', 'label', 'confidence']\n",
    "test = pd.read_csv('data/public.tsv', sep='\\t', quotechar = ' ', header = None)\n",
    "test.columns = ['context_id', 'context_2', 'context_1', 'context_0', 'reply_id', 'reply']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T20:08:49.697885Z",
     "start_time": "2018-04-12T20:08:49.688309Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T20:08:49.718919Z",
     "start_time": "2018-04-12T20:08:49.699054Z"
    },
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y - label, and prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T12:36:46.318173Z",
     "start_time": "2018-04-14T12:36:46.310014Z"
    },
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def label_enc(x ,reverse = False):\n",
    "    if reverse == False:\n",
    "        if x == 'bad':\n",
    "            return 0\n",
    "        elif x == 'neutral':\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "    else:\n",
    "        if x == 0:\n",
    "            return 'bad'\n",
    "        elif x == 1:\n",
    "            return 'neutral'\n",
    "        else:\n",
    "            return 'good'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T12:36:46.465305Z",
     "start_time": "2018-04-14T12:36:46.387800Z"
    },
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "df['label'] = df['label'].apply(lambda x: label_enc(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T12:36:46.744135Z",
     "start_time": "2018-04-14T12:36:46.570064Z"
    },
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T12:36:46.867452Z",
     "start_time": "2018-04-14T12:36:46.856534Z"
    },
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def DCG(label): return sum([float(label[i]/np.log2(i+2)) for i in range(len(label))])\n",
    "\n",
    "def nDCG(label, best_label):\n",
    "    label, best_label = DCG(label), DCG(best_label)\n",
    "    if label != 0 and best_label != 0:\n",
    "        return label/best_label\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "scorer = make_scorer(nDCG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Nan clearing\n",
    "Wanted more clever way to dell and fill nan, but dropna or fillna, will work good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T12:36:47.043056Z",
     "start_time": "2018-04-14T12:36:46.970645Z"
    },
    "hidden": true,
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "df.fillna('-', inplace=True)\n",
    "test.fillna('-', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T12:36:47.472128Z",
     "start_time": "2018-04-14T12:36:47.173115Z"
    },
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "import scipy.sparse as sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T12:36:47.652328Z",
     "start_time": "2018-04-14T12:36:47.646476Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def pre(s):\n",
    "    return re.sub(r'[^\\w]', ' ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T12:36:59.673875Z",
     "start_time": "2018-04-14T12:36:47.771613Z"
    },
    "hide_input": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def Vect(df, test, use_idf=True, min_df=1, max_df=1.0, ngram_range = (1,8)):\n",
    "    v = []\n",
    "    if use_idf == True:\n",
    "        tfidf = TfidfVectorizer(stop_words=None, preprocessor=pre,\n",
    "                               ngram_range=ngram_range, strip_accents='unicode', analyzer='word',\n",
    "                               min_df = min_df, max_df=max_df)\n",
    "    \n",
    "        context_0 = tfidf.fit_transform(df['context_0'])\n",
    "        context_0_t = tfidf.transform(test['context_0'])\n",
    "        v.append(tfidf)\n",
    "    \n",
    "        context_1 = tfidf.fit_transform(df['context_1'])\n",
    "        context_1_t = tfidf.transform(test['context_1'])\n",
    "        v.append(tfidf)\n",
    "        \n",
    "        context_2 = tfidf.fit_transform(df['context_2'])\n",
    "        context_2_t = tfidf.transform(test['context_1'])\n",
    "        v.append(tfidf)\n",
    "        \n",
    "        reply = tfidf.fit_transform(df['reply'])\n",
    "        reply_t = tfidf.transform(test['reply'])\n",
    "        v.append(tfidf)\n",
    "        \n",
    "        return sps.hstack((context_0, context_1, context_2, reply)), \\\n",
    "               sps.hstack((context_0_t, context_1_t, context_2_t, reply_t)), v\n",
    "    else:\n",
    "        tf = CountVectorizer(stop_words=None, preprocessor=pre,\n",
    "                             ngram_range=ngram_range, strip_accents='unicode', analyzer='word',\n",
    "                             min_df = min_df, max_df=max_df)\n",
    "\n",
    "        context_0 = tf.fit_transform(df['context_0'])\n",
    "        context_0_t = tf.transform(test['context_0'])\n",
    "        v.append(tf)\n",
    "    \n",
    "        context_1 = tf.fit_transform(df['context_1'])\n",
    "        context_1_t = tf.transform(test['context_1'])\n",
    "        v.append(tf)\n",
    "        \n",
    "        context_2 = tf.fit_transform(df['context_2'])\n",
    "        context_2_t = tf.transform(test['context_1'])\n",
    "        v.append(tf)\n",
    "        \n",
    "        reply = tf.fit_transform(df['reply'])\n",
    "        reply_t = tf.transform(test['reply'])\n",
    "        v.append(tf)\n",
    "    \n",
    "        return sps.hstack((context_0, context_1, context_2, reply)), \\\n",
    "               sps.hstack((context_0_t, context_1_t, context_2_t, reply_t)), v\n",
    "\n",
    "X_train_tf, X_test_tf, tf_m = Vect(df, test, use_idf=False, max_df=0.80, min_df=8)\n",
    "X_train_tfidf, X_test_tfidf, tf_m = Vect(df, test, max_df=0.80, min_df=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T08:20:22.351592Z",
     "start_time": "2018-04-13T08:20:22.347246Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T20:08:54.415546Z",
     "start_time": "2018-04-12T20:08:54.390344Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation,TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-04-12T20:08:47.517Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "\n",
    "def get_lda(data, test, topics):\n",
    "    lda = LatentDirichletAllocation(n_topics=topics, n_jobs=-1, learning_method='batch',\n",
    "                                   verbose = True).fit(data)\n",
    "    train = lda.transform(data)\n",
    "    test = lda.transform(test)\n",
    "    \n",
    "    return train, test, lda\n",
    "\n",
    "def get_kmeans(data, test, k, scale=True):\n",
    "    if scale == True:\n",
    "        scaler = MinMaxScaler().fit(data)\n",
    "        train = scaler.transform(data)\n",
    "        test = scale.transform(test)        \n",
    "    \n",
    "    kmean = KMeans(n_clusters=k).fit(data)\n",
    "    \n",
    "    train = kmean.predict(data)\n",
    "    test = kmean.predict(test)      \n",
    "    \n",
    "    return train, test, kmean    \n",
    "\n",
    "X_train_lda, X_train_lda, lda_m = get_lda(X_train_tf, X_test_tf, n_topics)\n",
    "X_train_m, X_test_m, kmean_m = get_kmeans(X_train_tfidf, X_test_tfidf, n_topics, scale=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### decomposion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-04-12T20:08:47.520Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-04-12T20:08:47.523Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def SVD(X_train, X_test):\n",
    "    svd = TruncatedSVD(n_components=4, n_iter = 50)\n",
    "    svd = TruncatedSVD().fit(X_train)\n",
    "    \n",
    "    X_train = sps.hstack((svd.transform(X_train), X_train))\n",
    "    X_test = sps.hstack((svd.transform(X_test), X_test))\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n",
    "X_train, X_test = SVD(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T13:51:00.034698Z",
     "start_time": "2018-04-08T13:51:00.031784Z"
    }
   },
   "source": [
    "### CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T12:53:34.039272Z",
     "start_time": "2018-04-25T12:53:33.914116Z"
    },
    "hide_input": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denis/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import VotingClassifier, AdaBoostClassifier, ExtraTreesClassifier, RandomForestClassifier, BaggingClassifier\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import datetime\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T12:53:35.127176Z",
     "start_time": "2018-04-25T12:53:35.107955Z"
    },
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_cv(X, y):\n",
    "    cv = StratifiedKFold(n_splits=6)\n",
    "    results = {\n",
    "        'lr': [],\n",
    "        'dtc': [],\n",
    "        'nb': [],\n",
    "        'xgb': [],\n",
    "        'lgb': [],\n",
    "        'cat': [],\n",
    "        'adb': [],\n",
    "        'etr': [],\n",
    "        'kn': [],\n",
    "        'rf': [],\n",
    "        'bag': [],\n",
    "        'sgd': [],\n",
    "        'combined': []\n",
    "    }\n",
    "    \n",
    "    lm = LogisticRegression()\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    nb = MultinomialNB()\n",
    "    xgb = XGBClassifier()\n",
    "    lgb = LGBMClassifier()\n",
    "    cat = CatBoostClassifier()\n",
    "    adb = AdaBoostClassifier()\n",
    "    etr = ExtraTreesClassifier()\n",
    "    kn = KNeighborsClassifier()\n",
    "    rf = RandomForestClassifier()\n",
    "    bag = BaggingClassifier()\n",
    "    sgd = SGDClassifier()\n",
    "    \n",
    "    vc = VotingClassifier([('lm', lm), ('dtc', dtc), ('nb', nb), \n",
    "                           ('xgb', xgb), ('adb', adb), ('etr', etr),\n",
    "                           ('kn', kn), ('rf', rf), ('bag', bag),\n",
    "                           ('sgd', sgd)])\n",
    "    \n",
    "    for c in tqdm_notebook([0,1,2]):\n",
    "        y_adj = np.array(y==c)\n",
    "        results['lr'].append((cross_val_score(lm, X, y_adj, cv=cv, scoring='accuracy', n_jobs=-1).mean(), c))\n",
    "        results['dtc'].append((cross_val_score(dtc, X, y_adj, cv=cv, scoring='accuracy', n_jobs=-1).mean(), c))\n",
    "        results['nb'].append((cross_val_score(nb, X, y_adj, cv=cv, scoring='accuracy', n_jobs=-1).mean(), c))\n",
    "        results['xgb'].append((cross_val_score(xgb, X, y_adj, cv=cv, scoring='accuracy', n_jobs=-1).mean(), c))\n",
    "        results['adb'].append((cross_val_score(adb, X, y_adj, cv=cv, scoring='accuracy', n_jobs=-1).mean(), c))\n",
    "        results['etr'].append((cross_val_score(etr, X, y_adj, cv=cv, scoring='accuracy', n_jobs=-1).mean(), c))\n",
    "        results['kn'].append((cross_val_score(kn, X, y_adj, cv=cv, scoring='accuracy', n_jobs=-1).mean(), c))\n",
    "        results['rf'].append((cross_val_score(rf, X, y_adj, cv=cv, scoring='accuracy', n_jobs=-1).mean(), c))\n",
    "        results['bag'].append((cross_val_score(bag, X, y_adj, cv=cv, scoring='accuracy', n_jobs=-1).mean(), c))\n",
    "        results['sgd'].append((cross_val_score(sgd, X, y_adj, cv=cv, scoring='accuracy', n_jobs=-1).mean(), c))\n",
    "        results['combined'].append((cross_val_score(vc, X, y_adj, cv=cv, scoring='accuracy', n_jobs=-1).mean(), c))\n",
    "    \n",
    "    print(\"Model accuracy predictions\\n\")\n",
    "    for m,s in list(results.items()):\n",
    "        for ss in s:\n",
    "            print((\"{M} model ({R} rating): {S:.1%}\".format(M=m.upper(), R=ss[1], S=ss[0])))\n",
    "            print()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T20:29:58.801551Z",
     "start_time": "2018-04-13T20:13:37.307296Z"
    },
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, df['label'], test_size=0.3)\n",
    "r1 = calculate_cv(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T20:57:44.755815Z",
     "start_time": "2018-04-13T20:43:49.752772Z"
    },
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tf, df['label'], test_size=0.3)\n",
    "r2 = calculate_cv(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T06:59:31.880976Z",
     "start_time": "2018-04-14T06:59:31.878846Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def decis(prob0, prob1, prob2):\n",
    "    if prob0 > prob1 and prob0 > prob2:\n",
    "        return [prob0, 0]\n",
    "    elif prob0 < prob1 and prob1 > prob2:\n",
    "        return [prob1, 1]\n",
    "    else:\n",
    "        return [prob2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T06:59:31.959113Z",
     "start_time": "2018-04-14T06:59:31.953716Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def predict(X_train, y ,X_test):\n",
    "    lm = LogisticRegression(class_weight='balanced')\n",
    "    dtc = DecisionTreeClassifier(class_weight='balanced')\n",
    "    nb = MultinomialNB()\n",
    "    xgb = XGBClassifier()\n",
    "    adb = AdaBoostClassifier()\n",
    "    etr = ExtraTreesClassifier(class_weight='balanced')\n",
    "    kn = KNeighborsClassifier(n_neighbors=10)\n",
    "    rf = RandomForestClassifier(class_weight='balanced')\n",
    "    bag = BaggingClassifier()\n",
    "    sgd = SGDClassifier(class_weight='balanced', loss='log', n_jobs=-1)\n",
    "    vc = VotingClassifier([('lm', lm), ('dtc', dtc), ('nb', nb), \n",
    "                           ('xgb', xgb), ('adb', adb), ('etr', etr),\n",
    "                           ('kn', kn), ('rf', rf), ('bag', bag),\n",
    "                           ('sgd', sgd)], n_jobs=-1, voting = 'soft')\n",
    "    \n",
    "    result = pd.DataFrame()\n",
    "    for c in tqdm_notebook([0,1,2]):\n",
    "        y_adj = np.array(y==c)\n",
    "        vc.fit(X_train, y_adj)\n",
    "        result['pred'+str(c)] = vc.predict(X_test)\n",
    "        result['proba'+str(c)] = vc.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    result['label'] = result.apply(lambda row: decis(row['proba0'], row['proba1'], row['proba2'])[1] ,axis=1)\n",
    "    result['confidence'] = result.apply(lambda row: decis(row['proba0'], row['proba1'], row['proba2'])[0] ,axis=1)\n",
    "    \n",
    "    return result[['label', 'confidence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T07:26:00.471115Z",
     "start_time": "2018-04-14T06:59:32.034524Z"
    },
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1dfe808d62476b90cfd5dfe1c88c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denis/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/denis/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/denis/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = predict(X_train_tfidf, df['label'], X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save subm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T07:26:00.581041Z",
     "start_time": "2018-04-14T07:26:00.522700Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "test['confidence'] = result['confidence']\n",
    "test['label']  = result['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-04-14T07:35:33.441Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "test.sort_values(by=['context_id', 'confidence'])[['context_id', 'reply_id']].to_csv('subm.csv', encoding='utf-8', sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
